model_name: meta-llama/Llama-2-7b-hf # TinyLlama/TinyLlama-1.1B-Chat-v1.0
# Note that the dtype for statistic profiling is hard-coded float32,
# since the SVD of float16 is not supported by pytorch
loqer_dtype: float32 # the precision for computing A, and B. Note that scale is always computed in float64
eval_dtype: bfloat16 # the precision for evaluation
device_map: auto-balanced
num_workers: 8
output_dir: null
disable_perplexity_eval: false
disable_lm_eval: false
overwrite_output_dir: null

# calibration & perplexity
AB_dict: null
calibration_set: "gsm8k"
num_calibration_samples: 256
perplexity_eval_batch_size: 4
perplexity_eval_set: "gsm8k"
perplexity_max_seq_length: null # not used for gsm8k
# lm-eval-harness
lm_eval_tasks:
  [
    "arc_easy",
    "lambada_openai",
    "piqa",
    "winogrande",
    "arc_challenge",
    "boolq",
    "openbookqa",
  ]
lm_eval_batch_size: 4
lm_eval_num_fewshot: 0

# LoQER
disable_loqer: false # if true, quantization only without A & B
loqer_scaling_mode: diag # "rxx" or "diagonal"
loqer_sqrtm_implementation: blocked # "blocked" or "iterative"
loqer_sqrtm_num_iters: 100
loqer_scaling_mode_map:
  # llama/mistral
  'model\.layers\.[0-9]+\.self_attn\.(k|q|v)_proj': rxx
  'model\.layers\.[0-9]+\.self_attn\.o_proj': diag
  'model\.layers\.[0-9]+\.mlp\.(gate|up)_proj': rxx
  'model\.layers\.[0-9]+\.mlp\.down_proj': diag
loqer_config:
  # llama/mistral
  'model\.layers\.[0-9]+\.self_attn\.(k|q|v|o)_proj': default-1
  'model\.layers\.[0-9]+\.mlp\.(gate|down|up)_proj': default-1
  'model\.layers\.[0-9]+\.self_attn\.(matmul_0|matmul_1)': default-matmul
  # opt
  'model\.decoder\.layers\.[0-9]+\.self_attn\.(k|q|v|out)_proj': default-1
  'model\.decoder\.layers\.[0-9]+\.(fc1|fc2)': default-1
  'model\.decoder\.layers\.[0-9]+\.self_attn\.(bmm_0|bmm_1)': default-matmul

  default-1:
    rank: 64
    name: loqer
    is_ptq: true
    x_quantizer:
      name: bypass
    w_quantizer:
      name: normalfloat
      num_bits: 4
      block_size: 64
    b_quantizer:
      name: bypass

  default-matmul:
    name: flexible
    x_quantizer:
      name: bypass
    w_quantizer:
      name: bypass

# fine-tuning config
fine_tuning: true
fine_tuning_config:
  model_name_or_path: LoftQ/Llama-2-7b-hf-4bit-64rank # TinyLlama/TinyLlama-1.1B-Chat-v1.0
  rank: 64 # rank for the lora adapter and this should be the same as the rank for the loqer
  # dataset
  dataset_name: gsm8k
  dataset_config_name: main
  pad_to_max_length: true
  max_source_length: 128
  max_target_length: 256
  with_tracking: true
  report_to: tensorboard
  eval_dtype: bfloat16

  ########################################
  # LoftQ Fine-tuning Config (shouldn't be changed)
  learning_rate: 3.0e-4
  weight_decay: 0.1
  lr_scheduler_type: cosine
  num_warmup_steps: 100
  seed: 202
  num_train_epochs: 6
  per_device_train_batch_size: 4
  per_device_eval_batch_size: 4
  gradient_accumulation_steps: 4
  ########################################



